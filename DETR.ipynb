{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a080c3-0f26-4284-a721-4e22c1add248",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_detr_path = \"shared/mldatasets/huggingface/hub/models--facebook--detr-resnet-50/snapshots/1d5f47bd3bdd2c4bbfa585418ffe6da5028b4c0b\"\n",
    "from transformers import DetrImageProcessor\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\n",
    "    local_detr_path,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            resnet = resnet50(weights='IMAGENET1K_V1')\n",
    "        except TypeError:\n",
    "            resnet = resnet50(pretrained=True)\n",
    "            \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            nn.Conv2d(1024, 256, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.feature_extractor(x)\n",
    "        return output\n",
    "\n",
    "class MovingObjectDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, frame_pairs_dir, annotation_dir, img_size=(800, 1333)):\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "        self.resize = Resize(img_size, interpolation=Image.BILINEAR)\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor().to(device)\n",
    "        self.feature_extractor.eval()\n",
    "        self.transform = Compose([\n",
    "            Resize(self.img_size),\n",
    "            ToTensor(),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.all_annotation_files = sorted([\n",
    "            os.path.join(self.annotation_dir, f)\n",
    "            for f in os.listdir(self.annotation_dir)\n",
    "            if f.endswith('.txt')\n",
    "        ])\n",
    "        \n",
    "        self.samples = []\n",
    "        annotation_file_idx = 0 \n",
    "        \n",
    "        for subdir_name in sorted(os.listdir(frame_pairs_dir)): \n",
    "            subdir_path = os.path.join(frame_pairs_dir, subdir_name)\n",
    "            if not os.path.isdir(subdir_path):\n",
    "                continue\n",
    "        \n",
    "            image_files_in_subdir = sorted([\n",
    "                os.path.join(subdir_path, f)\n",
    "                for f in os.listdir(subdir_path)\n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ])\n",
    "        \n",
    "            for i in range(0, len(image_files_in_subdir) - 1, 2):\n",
    "    \n",
    "                frame1_path = image_files_in_subdir[i]\n",
    "                frame2_path = image_files_in_subdir[i+1]\n",
    "                \n",
    "                current_ann_file = self.all_annotation_files[annotation_file_idx]\n",
    "        \n",
    "                self.samples.append((frame1_path, frame2_path, current_ann_file))\n",
    "                \n",
    "                annotation_file_idx += 1 \n",
    "        \n",
    "            if annotation_file_idx >= len(self.all_annotation_files):\n",
    "                break \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def parse_annotations(self, annotation_path):\n",
    "        objects = []\n",
    "        try:\n",
    "            with open(annotation_path) as f:\n",
    "                lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "            for i in range(0, len(lines), 2):\n",
    "                if i+1 >= len(lines):\n",
    "                    break\n",
    "                old = lines[i].split()\n",
    "                new = lines[i+1].split()\n",
    "                objects.append({\n",
    "                    'id': int(old[0]),\n",
    "                    'old_bbox': list(map(float, old[1:5])),\n",
    "                    'new_bbox': list(map(float, new[1:5])),\n",
    "                    'class': int(old[5])\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {annotation_path}: {e}\")\n",
    "        return objects\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame1_path, frame2_path, ann_path = self.samples[idx]\n",
    "\n",
    "        img1 = Image.open(frame1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(frame2_path).convert(\"RGB\")\n",
    "        \n",
    "        img1 = self.resize(img1)\n",
    "        img2 = self.resize(img2)\n",
    "        \n",
    "        diff = Image.fromarray(\n",
    "            np.abs(np.array(img1, dtype=np.int16) - np.array(img2, dtype=np.int16))\n",
    "            .astype(np.uint8)\n",
    "        )\n",
    "    \n",
    "        encoding = processor(images=diff, return_tensors=\"pt\", do_pad = True)\n",
    "        pixel_values = encoding.pixel_values.squeeze(0)  \n",
    "        pixel_mask   = encoding.pixel_mask.squeeze(0)   \n",
    "    \n",
    "        annotations = self.parse_annotations(ann_path)\n",
    "    \n",
    "        img_w, img_h = diff.size \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in annotations:\n",
    "            x0, y0, x1, y1 = obj['new_bbox']                 \n",
    "            cx = x0 / img_w\n",
    "            cy = y0 / img_h\n",
    "            w  = x1 / img_w\n",
    "            h  = y1 / img_h\n",
    "            boxes.append([cx, cy, w, h])\n",
    "            labels.append(obj['class'])\n",
    "    \n",
    "        targets = {\n",
    "            \"class_labels\":torch.tensor(labels, dtype=torch.long),\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"orig_size\": torch.tensor([img_h, img_w], dtype=torch.int64),\n",
    "            \"size\": torch.tensor([img_h, img_w], dtype=torch.int64),\n",
    "        }\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"pixel_mask\":   pixel_mask,\n",
    "            \"labels\":       targets\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4bb13-6ff0-4e4d-ad38-ed6b5db9e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    pixel_mask   = torch.stack([item[\"pixel_mask\"]   for item in batch])\n",
    "    labels       = [item[\"labels\"] for item in batch]\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"pixel_mask\":   pixel_mask,\n",
    "        \"labels\":       labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029989e0-6f62-4673-877e-01d968b1d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrForObjectDetection, DetrConfig\n",
    "from transformers import DetrImageProcessor\n",
    "\n",
    "local_detr_path = \"shared/mldatasets/huggingface/hub/models--facebook--detr-resnet-50/snapshots/1d5f47bd3bdd2c4bbfa585418ffe6da5028b4c0b\"\n",
    "config = DetrConfig.from_pretrained(local_detr_path)\n",
    "config.use_pretrained_backbone = False \n",
    "\n",
    "config.num_labels = 6 \n",
    "config.id2label = {\n",
    "    0: 'Unknown', 1: 'Person', 2: 'Car',\n",
    "    3: 'Other Vehicle', 4: 'Other Object', 5: 'Bike'\n",
    "} \n",
    "config.label2id = {v: k for k, v in config.id2label.items()}\n",
    "\n",
    "\n",
    "model = DetrForObjectDetection.from_pretrained(\n",
    "    local_detr_path,\n",
    "    config=config,\n",
    "    local_files_only=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73590a71-15d1-4632-b36d-af0d67bf9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_model(train_loader, val_loader, model, device, epochs=1):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            pixel_mask   = batch[\"pixel_mask\"].to(device)\n",
    "            targets = [\n",
    "                {k: v.to(device) for k, v in t.items()}\n",
    "                for t in batch[\"labels\"]\n",
    "            ]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                pixel_mask=  pixel_mask,\n",
    "                labels=      targets\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                pixel_mask   = batch[\"pixel_mask\"].to(device)\n",
    "                targets = [\n",
    "                    {k: v.to(device) for k, v in t.items()}\n",
    "                    for t in batch[\"labels\"]\n",
    "                ]\n",
    "                outputs = model(\n",
    "                    pixel_values = pixel_values,\n",
    "                    pixel_mask =   pixel_mask,\n",
    "                    labels =       targets\n",
    "                )\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} â€” \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58914e4b-8436-4130-931a-493e572b4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "def evaluate_and_visualize(model, dataset, device, idx=0, class_names=None, threshold=0.5):\n",
    "    model.eval()\n",
    "    item     = dataset[idx]\n",
    "    pix_vals = item[\"pixel_values\"].unsqueeze(0).to(device, non_blocking=True)\n",
    "    pix_mask = item[\"pixel_mask\"].unsqueeze(0).to(device, non_blocking=True)\n",
    "\n",
    "    _, _, H_processed, W_processed = pix_vals.shape\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pix_vals, pixel_mask=pix_mask)\n",
    "\n",
    "    logits      = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    scores, labels = logits.max(-1)          \n",
    "    boxes_norm  = outputs.pred_boxes[0]    \n",
    "\n",
    "    frame2_path = dataset.samples[idx][1]   \n",
    "    raw_img     = load_raw_image(frame2_path)  \n",
    "    H_orig, W_orig = raw_img.shape[:2]\n",
    "    \n",
    "    sx, sy = W_orig / W_processed, H_orig / H_processed\n",
    "   \n",
    "    cx, cy, w, h = boxes_norm.unbind(-1)\n",
    "    x_p = (cx - 0.5 * w) * W_processed\n",
    "    y_p = (cy - 0.5 * h) * H_processed\n",
    "    w_p = w * W_processed\n",
    "    h_p = h * H_processed\n",
    "\n",
    "    abs_boxes = torch.stack([\n",
    "        x_p * sx,\n",
    "        y_p * sy,\n",
    "        w_p * sx,\n",
    "        h_p * sy\n",
    "    ], dim=-1).cpu().numpy()\n",
    "   \n",
    "    scores_np = scores.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    keep = scores_np > threshold\n",
    "    abs_boxes = abs_boxes[keep]\n",
    "    labels_np = labels_np[keep]\n",
    "    scores_np = scores_np[keep]\n",
    "\n",
    "    plot_detections(\n",
    "        raw_img,\n",
    "        abs_boxes,\n",
    "        labels_np,\n",
    "        scores_np,\n",
    "        class_names=class_names,\n",
    "        threshold=threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531efc91-6744-4a87-aad9-6487dcc42c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    frame_pairs_dir = \"shared/data/cv_data_hw2/data\"\n",
    "    annotation_dir  = \"shared/hw3/matched_annotations\"\n",
    "    batch_size      = 2\n",
    "    device          = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = MovingObjectDataset(frame_pairs_dir, annotation_dir)\n",
    "    print(f\"Found {len(dataset)} valid image pairs\")\n",
    "    \n",
    "    n = len(dataset)\n",
    "    split = int(0.8 * n)\n",
    "    train_set, val_set = torch.utils.data.random_split(dataset, [split, n - split])\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    train_model(train_loader, val_loader, model, device, epochs=25)\n",
    "    output_dir = \"./detr_moving_checkpoint\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    evaluate_and_visualize(model, dataset, device, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633748d-98ac-49c0-b408-893024f5f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    print(f\"\\n--- Visualizing sample {idx} ---\")\n",
    "    evaluate_and_visualize(\n",
    "        model,\n",
    "        dataset,\n",
    "        device,\n",
    "        idx=idx,\n",
    "        class_names=[None, \"Person\", \"Car\", \"Other Vehicle\", \"Other Object\", \"Bike\"],\n",
    "        threshold=0.3,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:csciga-3033-spring]",
   "language": "python",
   "name": "conda-env-csciga-3033-spring-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
